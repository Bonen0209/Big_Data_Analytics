{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial the spark\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#this part is used for pyspark submit\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']='\\\n",
    "--verbose \\\n",
    "--master=yarn \\\n",
    "--deploy-mode=client \\\n",
    "pyspark-shell'\n",
    "\n",
    "os.environ['JAVA_HOME']='/usr/lib/jvm/java-8-openjdk-amd64/'\n",
    "os.environ['YARN_CONF_DIR']='/etc/alternatives/hadoop-conf/'\n",
    "\n",
    "#this line is used for spark1.6\n",
    "#os.environ['SPARK_HOME']='/opt/cloudera/parcels/CDH/lib/spark'\n",
    "\n",
    "#this line is used for spark2.2\n",
    "os.environ['SPARK_HOME']='/opt/cloudera/parcels/SPARK2-2.2.0.cloudera2-1.cdh5.12.0.p0.232957/lib/spark2'\n",
    "\n",
    "# this line is used for python2.7\n",
    "#os.environ['PYSPARK_PYTHON']='/usr/bin/python'\n",
    "\n",
    "#this line is used for python3.5\n",
    "os.environ['PYSPARK_PYTHON']='/usr/bin/python3'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.4-src.zip'))  \n",
    "#execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import to change the types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Read the file\n",
    "spWaferlog = spark.read \\\n",
    "             .format('csv') \\\n",
    "             .option('header', 'true') \\\n",
    "             .load('/user/homework_2/WaferLog.csv') \n",
    "\n",
    "spYield = spark.read \\\n",
    "          .format('csv') \\\n",
    "          .option('header', 'true') \\\n",
    "          .load('/user/homework_1/Yield.csv') \n",
    "\n",
    "#Change the schema\n",
    "spYield = spYield.withColumn('yield', spYield['yield'].cast(DoubleType()))\n",
    "\n",
    "#Rename the title\n",
    "spYield = spYield.withColumnRenamed('_c0','Wafer_ID')\n",
    "\n",
    "oldColumns = ['_c0', 'Lot.ID', 'Wafer.ID', 'Process.stage', 'Tool.ID']\n",
    "newColumns = ['number', 'Lot_ID', 'Wafer_ID', 'Process_stage', 'Tool_ID']\n",
    "\n",
    "for i in range(len(oldColumns)):\n",
    "    spWaferlog = spWaferlog.withColumnRenamed(oldColumns[i], newColumns[i])\n",
    "\n",
    "#Join the data\n",
    "spData = spYield.join(spWaferlog, 'Wafer_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import to have the Row\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#Evaluate the mean per Process_stage and Tool_ID\n",
    "spTool = spData.groupBy('Process_stage', 'Tool_ID').mean('yield').orderBy('Process_stage')\n",
    "\n",
    "#Evaluate the mean per Process_stage\n",
    "spProcess = spData.groupBy('Process_stage').mean('yield').orderBy('Process_stage')\n",
    "\n",
    "#Rename the title\n",
    "spTool = spTool.withColumnRenamed('avg(yield)', 'Average_Tool')\n",
    "spProcess = spProcess.withColumnRenamed('avg(yield)', 'Average_Process')\n",
    "\n",
    "#Collect from the spark dataframe\n",
    "Tool_IDs = spTool.collect()\n",
    "Process_stages = spProcess.rdd.collectAsMap()\n",
    "\n",
    "#Counting for the mean gap\n",
    "Mean_gaps = []\n",
    "\n",
    "for i in range(len(Tool_IDs)):\n",
    "    Mean_gaps.append(Row(Process_stage=Tool_IDs[i][0] \\\n",
    "                       , Tool_ID=Tool_IDs[i][1] \\\n",
    "                       , Mean_gap=abs(Tool_IDs[i][2] - Process_stages[Tool_IDs[i][0]])))\n",
    "    \n",
    "#Put back the dataframe into spark \n",
    "spResult = spark.createDataFrame(Mean_gaps)\n",
    "\n",
    "#Take back the top five value\n",
    "keys = spResult.orderBy('Mean_gap', ascending = False).take(5)\n",
    "\n",
    "#Pick the dataframe from the spData\n",
    "dfResults = []\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    dfResults.append(spData.where((spData.Process_stage == keys[i][1]) & (spData.Tool_ID == keys[i][2])) \\\n",
    "                           .select('yield', 'Process_stage', 'Tool_ID').toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the box plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(dfResults)):\n",
    "    plt.title('Process_stage ' + dfResults[i]['Process_stage'][0] + ' with Tool_ID ' + dfResults[i]['Tool_ID'][0])\n",
    "    plt.ylabel('Yield')\n",
    "    plt.boxplot(dfResults[i]['yield'])\n",
    "    plt.savefig('HW2_' + dfResults[i]['Process_stage'][0] + '_' + dfResults[i]['Tool_ID'][0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
